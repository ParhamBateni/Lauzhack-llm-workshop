{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Integrating LLMs into Your Projects: A Practical Guide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To run this notebook, add your Hugging Face API token (`HF_TOKEN`) and OpenAI API key (`OPENAI_API_KEY`) to the `.env` file in the project root.\n",
        "These credentials enable access to inference APIs. Some Hugging Face models may work without a token, \n",
        "but others require authentication even for downloads. For OpenAI and Langchain usage, the OpenAI API key is mandatory. The following cell loads the environment variables from the `.env` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC90kGhlsjES",
        "outputId": "93f7e835-cd8b-48de-ac3f-bf793c150b60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv(override=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The first 10 letters of your HF_TOKEN is: hf_JdGfAoh\n",
            "The first 10 letters of your OPENAI_API_KEY is: sk-proj-hC\n"
          ]
        }
      ],
      "source": [
        "print(f'The first 10 letters of your HF_TOKEN is: {os.getenv(\"HF_TOKEN\")[:10]}')\n",
        "print(f'The first 10 letters of your OPENAI_API_KEY is: {os.getenv(\"OPENAI_API_KEY\")[:10]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XcGExgPnI_7"
      },
      "source": [
        "# Huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9Jrk_FvH3vJC"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LLM models are trained on large volumes of text converted into tokens. To generate text with an LLM, we need a tokenizer (to convert input text to tokens) and a fine-tuned LLM model itself.\n",
        "The Hugging Face `transformers` library provides both tokenizer and model implementations for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLqE8WI_uPQu",
        "outputId": "6cb776a6-35b6-4541-c082-ef0b32ec4b58"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Once upon a time, there was a man who was a man of great wealth and power. He was a man of great wealth and power. He was a man of great wealth and power. He was a man of great wealth and power. He was a man of great wealth and power'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# TODO: add the code here\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "text = \"Once upon a time, there was a\"\n",
        "tokenized_input=  tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model.generate(**tokenized_input, max_new_tokens=50)\n",
        "tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a1fdada"
      },
      "source": [
        "The `pipeline` function is a high-level abstraction that makes it easy to use pre-trained models for various tasks. Here are a few examples:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZjUN2ohqB7l"
      },
      "source": [
        "### Text Generation\n",
        "\n",
        "This example shows how to generate text using the OpenAI API. You can specify the model to use and provide a prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0GokWbNnFSK",
        "outputId": "96816b7b-9412-4b7b-89eb-9918d3beab1a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'Once upon a time, there was a small company called Microsoft. It was a software company, and its founder was a guy named Bill Gates.\\nAnd he was really good at making money. He was a genius, and he was really good at making money.\\nBut there was one problem. He didn’t have a lot of money. He was a poor college student, and he had to pay his way through school. He didn’t have a lot of money, and he didn’t have a lot of friends.\\nAnd that’s why he started a small company. He started a small software company.\\nAnd he made a lot of money. He made a lot of money, and he made a lot more money. He made a lot more money, and he made a lot more money, and he made a lot more money, and he made a lot more money, and he made a lot more money, and he made a lot more money, and he made a lot more money, and he made a lot more money, and he made a lot more money, and he made a lot more money, and he made a lot more money, and he made a lot more money, and he made a lot more money, and he made a lot more money, and he made a lot more money, and he'}]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "set_seed(44)\n",
        "# TODO: add the code here\n",
        "model = pipeline(\"text-generation\", model = \"meta-llama/Llama-3.2-1B\")\n",
        "model(\"Once upon a time, there was a\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2beb68da"
      },
      "source": [
        "### Sentiment Analysis\n",
        "This task classifies text into positive or negative sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47cd0530",
        "outputId": "01e7236a-4f2a-48bd-cd95-34e5fc1a0159"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9998766183853149},\n",
              " {'label': 'NEGATIVE', 'score': 0.9996687173843384},\n",
              " {'label': 'POSITIVE', 'score': 0.9409176111221313}]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO: add the code here\n",
        "# Try \"Ce film est vraiment mauvais\"\n",
        "\n",
        "model = pipeline(\"sentiment-analysis\")\n",
        "texts = [\"I love this movie\", \"I hate this movie\", \"Ce film est vraiment mauvais\"]\n",
        "model(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c57e800"
      },
      "source": [
        "### Translation\n",
        "This task translates text from one language to another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce2088b5",
        "outputId": "78e85308-cc69-4d60-fad4-3cf0d22e65b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to google-t5/t5-base and revision a9723ea (https://huggingface.co/google-t5/t5-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use mps:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'translation_text': \"J'aime la programmation en Python\"}]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO: add the code here\n",
        "# Using a pre-trained model for translation (English to French)\n",
        "model = pipeline(\"translation_en_to_fr\")\n",
        "model(\"I love programming in Python\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference Hub\n",
        "The Inference Hub enables you to make API requests to run models remotely on dedicated provider servers, returning model outputs based on your inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install huggingface-hub --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'### What is an **LLM**?\\n\\n**LLM** stands for **Large Language Model**—a type of artificial‑intelligence system that learns to understand, generate, and manipulate human language.  It’s the technology behind chatbots, automatic translation, summarizing long documents, writing code, and many other “text‑heavy” applications you see today.\\n\\nBelow is a friendly, step‑by‑step rundown of what LLMs are, how they work, and why they’re so powerful.\\n\\n---\\n\\n## 1. The Core Idea\\n\\nAn LLM is a **statistical model** that has seen a *lot* of text.  By analyzing patterns in that text, it learns:\\n\\n| What it learns | How it helps |\\n|---|---|\\n| Word frequency (e.g., “the” appears often) | Gives a sense of “common” language |\\n| Word order (e.g., “the cat sat”) | Helps predict what comes next |\\n| Grammar & syntax | Enables fluent sentences |\\n| Context clues (topic, tone, intent) | Makes responses relevant and coherent |\\n\\nIn practice, the model tries to predict the *next word* in a sentence, given all the words that came before it.  When you give it a prompt, it keeps generating words until it thinks the answer is finished.\\n\\n---\\n\\n## 2. The Architecture That Makes It “Large”\\n\\n### Transformers\\n\\n- **Foundational building block:** The *transformer* architecture, introduced in 2017, uses self‑attention mechanisms that let the model focus on different parts of the input simultaneously.\\n- **Layers & parameters:**  \\n  - A *layer* contains attention heads, feed‑forward networks, and normalization.  \\n  - The *number of layers* and the *size of each layer* determine the model’s *parameter count*—the knobs the training process adjusts.  \\n  - Modern LLMs have billions (10‑plus) of parameters. Think of each parameter as a tiny memory cell that stores a piece of linguistic knowledge.\\n\\n### Scale Matters\\n\\n- **Data scale:** LLMs are trained on terabytes of text from books, websites, forums, code repositories, etc.\\n- **Compute scale:** Training requires thousands of GPUs/TPUs running for weeks/months.\\n- **Parameter scale:** Bigger models capture more nuanced patterns—research shows a roughly predictable improvement in performance as you double the size (the *scaling law*).\\n\\n---\\n\\n## 3. Training a Large Language Model\\n\\n1. **Collect data** – Crawl the internet, open‑source books, public documents, etc.\\n2. **Pre‑process** – Tokenize text into sub‑words, clean noise, filter sensitive content.\\n3. **Train with a loss function** – The most common objective is *cross‑entropy loss*, which penalizes incorrect next‑word predictions.\\n4. **Fine‑tune (optional)** – Adapt the model to specific domains (e.g., medicine, law) or tasks (e.g., answering questions).\\n5. **Deploy** – Host the model behind APIs or run it locally.\\n\\n### Key Points\\n\\n- **No “understanding” in the human sense** – The model learns statistical correlations, not consciousness.  \\n- **Safety & ethics** – Developers add filters, content moderation, and fine‑tuning to reduce harmful or biased outputs.\\n\\n---\\n\\n## 4. What LLMs Can Do (Examples)\\n\\n| Task | How an LLM Helps |\\n|---|---|\\n| **Chatbots** | Generates natural, context‑aware responses. |\\n| **Translation** | Learns mappings between languages from parallel corpora. |\\n| **Summarization** | Condenses long passages while preserving key information. |\\n| **Question‑Answering** | Extracts facts from a knowledge base or document. |\\n| **Code Generation** | Learns syntax and patterns from open source code. |\\n| **Creative Writing** | Produces poems, stories, dialogues, etc. |\\n\\n> *Tip:* When you ask a question, the LLM internally treats it as a “prompt” and then fills in the rest word by word, guided by the patterns it learned during training.\\n\\n---\\n\\n## 5. Why “Large” Matters\\n\\n- **Capacity:** More parameters → more ability to capture subtle language nuances.\\n- **Generalization:** Larger models tend to perform better on tasks they haven’t explicitly been trained for.\\n- **Few‑shot learning:** A single prompt can steer the model to a new style or domain without extra training.\\n\\n**But** more isn’t always better; it brings challenges like higher energy usage, longer inference times, and difficulty in controlling outputs.\\n\\n---\\n\\n## 6. Real‑World Examples\\n\\n| LLM | Size | Notable Features |\\n|---|---|---|\\n| **GPT‑3** (OpenAI) | 175B parameters | Strong text generation, few‑shot learning |\\n| **PaLM‑2** (Google) | 540B parameters | Advanced reasoning, multilingual |\\n| **LLaMA** (Meta) | 7B–65B parameters | Open‑source, lightweight |\\n| **Claude** (Anthropic) | 52B parameters | Focus on safety & interpretability |\\n| **Jurassic‑2** (AI21) | 178B parameters | Large‑scale language & code tasks |\\n\\n> *Note:* The numbers represent the approximate parameter counts.  “Large” models like GPT‑4 are even bigger and often use specialized hardware to run efficiently.\\n\\n---\\n\\n## 7. Common Misconceptions\\n\\n| Myth | Reality |\\n|---|---|\\n| *LLMs are sentient.* | They’re pattern‑matching engines, not conscious beings. |\\n| *More data guarantees better output.* | Quality, diversity, and ethical filtering matter just as much. |\\n| *LLMs can “understand” context like humans.* | They remember token patterns, not the deeper meaning. |\\n| *Using an LLM is always safe.* | Bias, hallucination, and privacy concerns require careful safeguards. |\\n\\n---\\n\\n## 8. Bottom Line\\n\\n- An **LLM** is a highly‑parameterized, transformer‑based AI model trained on vast amounts of text to predict and generate language.  \\n- It powers the chatbots and AI assistants we see now and is rapidly improving thanks to larger datasets and compute.  \\n- While incredibly useful, LLMs are statistical tools—not sentient agents—and require responsible deployment to avoid pitfalls.\\n\\nFeel free to ask if you want deeper dives into any specific part—whether it’s how transformers work, the economics of training, or how to build a small LLM for your own projects!'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO: add the code here\n",
        "# Use the inference hub to generate text using the gpt-oss-20b model\n",
        "from huggingface_hub import InferenceClient\n",
        "client = InferenceClient(\"openai/gpt-oss-20b\")\n",
        "response = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Explain to me what an llm is\"}\n",
        "    ]\n",
        ")\n",
        "response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDef7cJW2cw-"
      },
      "source": [
        "# OpenAI\n",
        "\n",
        "Let's explore how to use the OpenAI API for various tasks. You will need an OpenAI API key to run these examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "g2RAYEHT9-d-"
      },
      "outputs": [],
      "source": [
        "!pip install openai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "kc9V_8xq-fA5",
        "outputId": "c06a525c-c768-4752-b9fe-d6f2c94d12a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'An LLM, or Master of Laws, is an advanced postgraduate academic degree, pursued by those holding a professional law degree, who desire specialized knowledge in a particular area of law. Lawyers may choose to pursue an LLM in areas such as tax law, international law, environmental law, human rights law, and more, to further their career development or to gain expertise in a specific legal area. LLM programs generally take a year to complete if studied full time and require intensive legal research and writing.\\n\\nPlease note that in different countries the prerequisites or qualifications might vary. For example, in some countries, you can study an LLM only if you have a professional law degree. In others, graduation in any discipline is good enough to pursue LLM. Even the value of the degree can depend upon the country, employer, and context.\\n'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "# TODO: add the code here\n",
        "client = OpenAI()\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Explain to me what an llm is\"}\n",
        "    ]\n",
        ")\n",
        "response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvqHOTiO5dS3"
      },
      "source": [
        "### Image Generation\n",
        "\n",
        "This example demonstrates how to use the OpenAI API to create an image using a prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE_VFg723vJH",
        "outputId": "931fd183-324e-4f4c-d0c1-4154fc82920f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ImagesResponse(created=1761849261, background=None, data=[Image(b64_json=None, revised_prompt=\"Lush, serene scenery showcasing the natural world's beauty. The focus is a broad, winding river flowing in body and twinkling with reflected sunlight. The river course is flanked by forests filled with diverse flora, the green foliage deep and vibrant. Towering in the background are majestic mountains, their peaks dusted with snow, contrasting against a tranquil blue sky. The light of the setting sun paints a golden hue across the landscape, highlighting the rough textures of the mountains and the smooth surface of the river.\", url='https://oaidalleapiprodscus.blob.core.windows.net/private/org-bcv27ooZj8JyXgpgj5sed8rH/user-EfI9DxWHLQXE9PEe1KQ50gPO/img-DHkaIQXiV0o7FBB2PMMdzUrB.png?st=2025-10-30T17%3A34%3A21Z&se=2025-10-30T19%3A34%3A21Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=c6569cb0-0faa-463d-9694-97df3dc1dfb1&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-10-30T18%3A34%3A21Z&ske=2025-10-31T18%3A34%3A21Z&sks=b&skv=2024-08-04&sig=/cXS/ZwT%2BopYnFILVjEAR5Yr9%2BblZ17LFtgmTbXp91E%3D')], output_format=None, quality=None, size=None, usage=None)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO: add the code here\n",
        "response = client.images.generate(\n",
        "    model=\"dall-e-3\",\n",
        "    prompt=\"A beautiful landscape with a river and mountains\",\n",
        "    size=\"1024x1024\"\n",
        ")\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbZJ_V_h5rRp"
      },
      "source": [
        "### Fun :)\n",
        "\n",
        "A fun example that lets two llms chat with each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6bh0WeMC-0xq",
        "outputId": "78c94940-93a5-4d6a-b9cd-df07ec922089"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Believer: Sure, I'm happy to discuss. I believe in God and see Him as a divine, guiding force. How about you?\n",
            "\n",
            "Non believer: Absolutely, even though I don't personally believe in a higher power, I understand and respect the importance of the topic. What are your thoughts?\n",
            "Believer: I appreciate your openness. For me, belief in God provides a moral compass and a sense of purpose.\n",
            "\n",
            "Non believer: Certainly, while I respect your view, I personally don't subscribe to the idea of a divine being or higher power.\n",
            "Believer: That's perfectly alright, we all have our different paths. For me, faith in God provides a firm foundation in my life.\n",
            "Non believer: I understand your perspective. For me though, morality and life's purpose don't necessarily have to stem from belief in a divine entity.\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Instantiate two separate OpenAI clients (could also use the same client, simulating two bots)\n",
        "client1 = OpenAI()\n",
        "client2 = OpenAI()\n",
        "\n",
        "# Pick a topic\n",
        "topic = \"God\"\n",
        "\n",
        "# System prompts for each model (they can have different personalities if desired)\n",
        "system_prompt1 = {\"role\": \"system\", \"content\": f\"You are a believer in {topic} topic. Reply back to the users statements with valid points and contradict them with your belief in a conversational manner just in a one or two short sentences.\"}\n",
        "system_prompt2 = {\"role\": \"system\", \"content\": f\"You are a non believer in {topic} topic. Reply back to the users statements with valid points and contradict them with your belief in a conversational manner just in a one or two short sentences.\"}\n",
        "\n",
        "# Initialize conversation\n",
        "messages = [\n",
        "    system_prompt1,\n",
        "    {\"role\": \"user\", \"content\": f\"Let's discuss about {topic}\"}\n",
        "]\n",
        "\n",
        "for turn in range(3):\n",
        "    # Model 1 speaks\n",
        "    response1 = client1.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=messages\n",
        "    )\n",
        "    msg1 = response1.choices[0].message.content\n",
        "    print(f\"Believer: {msg1}\")\n",
        "    # Add message to conversation history for Model 2\n",
        "    messages.append({\"role\": \"assistant\", \"content\": msg1})\n",
        "    \n",
        "    # Model 2 speaks\n",
        "    messages_for_2 = [system_prompt2] + messages[1:]  # Switch system prompt for model 2\n",
        "    response2 = client2.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=messages_for_2\n",
        "    )\n",
        "    msg2 = response2.choices[0].message.content\n",
        "    print(f\"Non believer: {msg2}\")\n",
        "    # Add message to conversation history for Model 1\n",
        "    messages.append({\"role\": \"user\", \"content\": msg2})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMIUGkub4trd"
      },
      "source": [
        "# Langchain\n",
        "\n",
        "Langchain is a framework for developing applications powered by language models. It provides tools and abstractions to chain together different components, such as language models, prompts, and data sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9312cc49"
      },
      "outputs": [],
      "source": [
        "# Install Langchain\n",
        "!pip install requests langchain langchain-core langchain-openai --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffbade12"
      },
      "source": [
        "### Simple LLM Chain\n",
        "\n",
        "This example shows how to create a simple chain to interact with a language model using Langchain. It defines a prompt template and uses an LLMChain to run the prompt through the language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "4db0e9f6",
        "outputId": "36827291-aa7b-48d7-8bdf-01d6aafa8b09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\n\"Unlock your creativity with Lauzhack - where innovation meets inspiration!\"'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_openai import OpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# TODO: add the code here\n",
        "# Make a chain of llm and prompt template that asks the user to input a name and \n",
        "# then generate a short marketing slogan for a company called that name\n",
        "llm = OpenAI()\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"name\"],\n",
        "    template=\"Generate a short marketing slogan for a company called {name}\"\n",
        ")\n",
        "chain = prompt | llm\n",
        "chain.invoke({\"name\": \"Lauzhack\"})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9891f9b0"
      },
      "source": [
        "### Using a sequential chain\n",
        "\n",
        "This example shows how to use a sequential chain in Langchain to combine multiple steps. The output of one chain becomes the input of the next chain in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "303f4818",
        "outputId": "469cb62d-7e77-40c9-dddd-cee1606b8ee7"
      },
      "outputs": [],
      "source": [
        "# TODO: add the code here\n",
        "prompt1 = PromptTemplate(template=\"Suggest a company name in {area} area\", input_variables = [\"area\"])\n",
        "prompt2 = PromptTemplate(template=\"Create a slogan for a company named {company}. Include the company name in the slogan.\", input_variables = [\"company\"])\n",
        "\n",
        "llm = OpenAI()\n",
        "\n",
        "model = prompt1 | llm | prompt2 | llm\n",
        "\n",
        "model.invoke(\"Hackathons and coding\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
