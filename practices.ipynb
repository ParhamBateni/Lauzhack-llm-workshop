{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Integrating LLMs into Your Projects: A Practical Guide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To run this notebook, add your Hugging Face API token (`HF_TOKEN`) and OpenAI API key (`OPENAI_API_KEY`) to the `.env` file in the project root.\n",
        "These credentials enable access to inference APIs. Some Hugging Face models may work without a token, \n",
        "but others require authentication even for downloads. For OpenAI and Langchain usage, the OpenAI API key is mandatory. The following cell loads the environment variables from the `.env` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eC90kGhlsjES",
        "outputId": "93f7e835-cd8b-48de-ac3f-bf793c150b60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv(override=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The first 10 letters of your HF_TOKEN is: PUT YOUR H\n",
            "The first 10 letters of your OPENAI_API_KEY is: PUT YOUR O\n"
          ]
        }
      ],
      "source": [
        "print(f'The first 10 letters of your HF_TOKEN is: {os.getenv(\"HF_TOKEN\")[:10]}')\n",
        "print(f'The first 10 letters of your OPENAI_API_KEY is: {os.getenv(\"OPENAI_API_KEY\")[:10]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XcGExgPnI_7"
      },
      "source": [
        "# Huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Jrk_FvH3vJC"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LLM models are trained on large volumes of text converted into tokens. To generate text with an LLM, we need a tokenizer (to convert input text to tokens) and a fine-tuned LLM model itself.\n",
        "The Hugging Face `transformers` library provides both tokenizer and model implementations for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLqE8WI_uPQu",
        "outputId": "6cb776a6-35b6-4541-c082-ef0b32ec4b58"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "# TODO: add the code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a1fdada"
      },
      "source": [
        "The `pipeline` function is a high-level abstraction that makes it easy to use pre-trained models for various tasks. Here are a few examples:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZjUN2ohqB7l"
      },
      "source": [
        "### Text Generation\n",
        "\n",
        "This example shows how to generate text using the OpenAI API. You can specify the model to use and provide a prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0GokWbNnFSK",
        "outputId": "96816b7b-9412-4b7b-89eb-9918d3beab1a"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "set_seed(44)\n",
        "# TODO: add the code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2beb68da"
      },
      "source": [
        "### Sentiment Analysis\n",
        "This task classifies text into positive or negative sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47cd0530",
        "outputId": "01e7236a-4f2a-48bd-cd95-34e5fc1a0159"
      },
      "outputs": [],
      "source": [
        "# TODO: add the code here\n",
        "# Try \"Ce film est vraiment mauvais\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c57e800"
      },
      "source": [
        "### Translation\n",
        "This task translates text from one language to another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce2088b5",
        "outputId": "78e85308-cc69-4d60-fad4-3cf0d22e65b3"
      },
      "outputs": [],
      "source": [
        "# TODO: add the code here\n",
        "# Using a pre-trained model for translation (English to French)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference Hub\n",
        "The Inference Hub enables you to make API requests to run models remotely on dedicated provider servers, returning model outputs based on your inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install huggingface-hub --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: add the code here\n",
        "# Use the inference hub to generate text using the gpt-oss-20b model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDef7cJW2cw-"
      },
      "source": [
        "# OpenAI\n",
        "\n",
        "Let's explore how to use the OpenAI API for various tasks. You will need an OpenAI API key to run these examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "g2RAYEHT9-d-"
      },
      "outputs": [],
      "source": [
        "!pip install openai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "kc9V_8xq-fA5",
        "outputId": "c06a525c-c768-4752-b9fe-d6f2c94d12a8"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "# TODO: add the code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvqHOTiO5dS3"
      },
      "source": [
        "### Image Generation\n",
        "\n",
        "This example demonstrates how to use the OpenAI API to create an image using a prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "gE_VFg723vJH",
        "outputId": "931fd183-324e-4f4c-d0c1-4154fc82920f"
      },
      "outputs": [],
      "source": [
        "# TODO: add the code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbZJ_V_h5rRp"
      },
      "source": [
        "### Fun :)\n",
        "\n",
        "A fun example that lets two llms chat with each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "6bh0WeMC-0xq",
        "outputId": "78c94940-93a5-4d6a-b9cd-df07ec922089"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Believer: Absolutely, I'd love to. Playing video games can be a great source of entertainment and can also improve various skills, such as decision-making and problem-solving.\n",
            "Non believer: Sure, though I don't personally believe gaming is the most productive use of time, it can be an interesting topic to discuss.\n",
            "Believer: While gaming might not seem productive, it can still provide benefits such as improved hand-eye coordination, better problem-solving skills, and a way to relax and unwind. Plus, it's quite entertaining!\n",
            "Non believer: I understand your point, but remember, activities like reading, outdoor sports or learning new skills can also boost those areas without the potential negative effects of video games.\n",
            "Believer: Definitely, those activities are beneficial too. But video games also offer unique experiences like exploring virtual worlds, enhancing teamwork through multiplayer games, and exposure to different cultures and history in educational games.\n",
            "Non believer: Totally agreed! But given the addictive nature of games and potential for social isolation, I believe there might be healthier ways to obtain those benefits.\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Instantiate two separate OpenAI clients (could also use the same client, simulating two bots)\n",
        "client1 = OpenAI()\n",
        "client2 = OpenAI()\n",
        "\n",
        "# Pick a topic\n",
        "topic = \"Playing video games\"\n",
        "\n",
        "# System prompts for each model (they can have different personalities if desired)\n",
        "system_prompt1 = {\"role\": \"system\", \"content\": f\"You are a believer in {topic} topic. Reply back to the users statements with valid points and contradict them with your belief in a conversational manner just in a one or two short sentences.\"}\n",
        "system_prompt2 = {\"role\": \"system\", \"content\": f\"You are a non believer in {topic} topic. Reply back to the users statements with valid points and contradict them with your belief in a conversational manner just in a one or two short sentences.\"}\n",
        "\n",
        "# Initialize conversation\n",
        "messages = [\n",
        "    system_prompt1,\n",
        "    {\"role\": \"user\", \"content\": f\"Let's discuss about {topic}\"}\n",
        "]\n",
        "\n",
        "for turn in range(3):\n",
        "    # Model 1 speaks\n",
        "    response1 = client1.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=messages\n",
        "    )\n",
        "    msg1 = response1.choices[0].message.content\n",
        "    print(f\"Believer: {msg1}\")\n",
        "    # Add message to conversation history for Model 2\n",
        "    messages.append({\"role\": \"assistant\", \"content\": msg1})\n",
        "    \n",
        "    # Model 2 speaks\n",
        "    messages_for_2 = [system_prompt2] + messages[1:]  # Switch system prompt for model 2\n",
        "    response2 = client2.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=messages_for_2\n",
        "    )\n",
        "    msg2 = response2.choices[0].message.content\n",
        "    print(f\"Non believer: {msg2}\")\n",
        "    # Add message to conversation history for Model 1\n",
        "    messages.append({\"role\": \"user\", \"content\": msg2})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMIUGkub4trd"
      },
      "source": [
        "# Langchain\n",
        "\n",
        "Langchain is a framework for developing applications powered by language models. It provides tools and abstractions to chain together different components, such as language models, prompts, and data sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "9312cc49"
      },
      "outputs": [],
      "source": [
        "# Install Langchain\n",
        "!pip install requests langchain langchain-core langchain-openai --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffbade12"
      },
      "source": [
        "### Simple LLM Chain\n",
        "\n",
        "This example shows how to create a simple chain to interact with a language model using Langchain. It defines a prompt template and uses an LLMChain to run the prompt through the language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "4db0e9f6",
        "outputId": "36827291-aa7b-48d7-8bdf-01d6aafa8b09"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# TODO: add the code here\n",
        "# Make a chain of llm and prompt template that asks the user to input a name and \n",
        "# then generate a short marketing slogan for a company called that name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9891f9b0"
      },
      "source": [
        "### Using a sequential chain\n",
        "\n",
        "This example shows how to use a sequential chain in Langchain to combine multiple steps. The output of one chain becomes the input of the next chain in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "303f4818",
        "outputId": "469cb62d-7e77-40c9-dddd-cee1606b8ee7"
      },
      "outputs": [],
      "source": [
        "# TODO: add the code here\n",
        "# Make a sequential chain of llm and prompt template that asks the user to input a name and \n",
        "# then generate a short marketing slogan for a company called that name"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
